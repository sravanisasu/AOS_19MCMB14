{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_2013_org.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMV1o6H9mRzT5MzichOObQC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sravanisasu/volatality_pred/blob/master/CNN_2013_org.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdzXgEEOy09k",
        "outputId": "0cca5530-f3cd-4727-dbe4-7d2e3236ef04"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqQcws1D67SG"
      },
      "source": [
        "year = 2013"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjYJWpDrn7jQ"
      },
      "source": [
        "**Required imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-MWsoiWOC8J",
        "outputId": "15ce6855-8d8f-4d4e-e7ef-e9e42c45cb6e"
      },
      "source": [
        "# Importing the required packages\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "import numpy as np\n",
        "from numpy import random\n",
        "from pickle import load\n",
        "from numpy import array\n",
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Embedding\n",
        "from keras import optimizers\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.layers.merge import concatenate\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.metrics import RootMeanSquaredError\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.layers.advanced_activations import LeakyReLU"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RGtsu1_n_kF"
      },
      "source": [
        "**building embedding for the words**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7M9JpeG0CYa"
      },
      "source": [
        "#Define file paths required for the model\n",
        "\n",
        "# embedding bin file\n",
        "embed_file = \"/content/drive/MyDrive/10-K dataset/sim.expand.200d.vec\"\n",
        "\n",
        "#Define Hyper parameters\n",
        "max_inp_len = 20000\n",
        "# the dimension of vectors to be used\n",
        "embed_dim = 200\n",
        "rounding = 6\n",
        "# filter sizes of the different conv layers \n",
        "filter_sizes = [3,4,5]\n",
        "num_filters = 1\n",
        "pool_size = 199\n",
        "# dropout probability\n",
        "drop = 0.5\n",
        "batch_size = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "epochs = 30"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DbBN4tosBE9",
        "outputId": "f19251ac-d2fc-4401-ab87-b8581c93e868"
      },
      "source": [
        "#define embedding dictionary and embed matrix for the vocabulary\n",
        "embeddings_dic = dict()\n",
        "f = open(embed_file,encoding='utf8')\n",
        "with open(embed_file, 'r', encoding='utf-8') as e_file:\n",
        "  for line in e_file:\n",
        "    splitlines = line.split()\n",
        "    word = splitlines[0].strip()\n",
        "    coefs = np.asarray(splitlines[1:], dtype='float32')\n",
        "    embeddings_dic[word] = coefs\n",
        "\n",
        "print(\"length of embedding dictionary\",len(embeddings_dic))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "length of embedding dictionary 70429\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqa1tA-lt6e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d81a112-9ee1-4390-e916-7ff3ee4f1375"
      },
      "source": [
        "vocabulary_size = len(embeddings_dic.keys())\n",
        "embed_token = Tokenizer()\n",
        "embed_token.fit_on_texts(embeddings_dic.keys())\n",
        "embedding_matrix = np.zeros((vocabulary_size, embed_dim))\n",
        "for word, index in embed_token.word_index.items():\n",
        "  embedding_matrix[index] = embeddings_dic.get(word)\n",
        "print(\"embedding_matrix dimension\",len(embedding_matrix),len(embedding_matrix[0]))\n",
        "print(\"no of token in the tokenizer\",len(embed_token.word_index) + 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "embedding_matrix dimension 70429 200\n",
            "no of token in the tokenizer 70429\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Rm4k5CZoOau"
      },
      "source": [
        "**Pre processing input and output**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "laF6m-NotXFw"
      },
      "source": [
        "#function to pre process the document\n",
        "def process_doc(path_file,embed_token) :\n",
        "\n",
        "  #tokenizing the words \n",
        "  with open(path_file,'r', encoding='utf-8') as tok_file :\n",
        "    file_words = list(tok_file)[0].split()\n",
        "    \n",
        "  #removing the stop words\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  filtered_words = []  \n",
        "  for word in file_words: \n",
        "      if word not in stop_words and word.isalpha(): \n",
        "          filtered_words.append(word)\n",
        "\n",
        "  # applying stemming using PorterStemmer\n",
        "\n",
        "  p_stemmer = PorterStemmer()\n",
        "  stem_words=[]\n",
        "  for word in filtered_words:\n",
        "    stem_words.append(p_stemmer.stem(word))\n",
        "    \n",
        "  #tokenizing the words using the embed token\n",
        "  tokens=[]\n",
        "  for word in stem_words:\n",
        "    try:\n",
        "      tokens.append(embed_token.word_index[word])\n",
        "    except:\n",
        "      tokens.append(1)\n",
        "\n",
        "  if len(tokens) < max_inp_len:\n",
        "    tokens.extend([0]*(max_inp_len-len(tokens)))\n",
        "  else:\n",
        "    tokens = tokens[:max_inp_len]\n",
        "    \n",
        "  return np.array(tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NCdDHfYt3l7"
      },
      "source": [
        "#output dataset\n",
        "def output_data(company_id, out_path_file):\n",
        "  with open(out_path_file,'r', encoding='utf-8') as out_file :\n",
        "    for line in out_file.readlines():\n",
        "      if company_id == line.split()[1]:\n",
        "        return line.split()[0]\n",
        "  return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BctWiw3UixYv"
      },
      "source": [
        "def pre_processing(meta_file,output_file,year):\n",
        "\n",
        "  with open(meta_file,'r', encoding='utf-8') as m_file :\n",
        "    \n",
        "    dir_path = '/content/drive/MyDrive/10-K dataset/all.tok' + '/' +str(year)+'.tok'\n",
        "    data =[]\n",
        "\n",
        "    for line in m_file.readlines():\n",
        "      inp_path_file = dir_path +'/'+ line.split()[0] + '.mda'\n",
        "      \n",
        "      # get input tokens from the company document\n",
        "      inp_tokens = process_doc(inp_path_file,embed_token)\n",
        "      \n",
        "      # get output value for the company\n",
        "      out_values = output_data(line.split()[0],output_file)\n",
        "\n",
        "      #insert values into the data list\n",
        "      data.append({'token':inp_tokens,'value':out_values})\n",
        "\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVi-HHNCPHkz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02f456b1-a716-4498-dc80-0f58b35f1c14"
      },
      "source": [
        "######## extracting text and storing it in dataframes ########\n",
        "data_train = pre_processing('/content/drive/MyDrive/10-K dataset/all.meta/'+str(year-3)+'.meta.txt','/content/drive/MyDrive/10-K dataset/all.logfama/'+str(year-3)+'.logfama.txt',year-3)\n",
        "data_train.extend(pre_processing('/content/drive/MyDrive/10-K dataset/all.meta/'+str(year-2)+'.meta.txt','/content/drive/MyDrive/10-K dataset/all.logfama/'+str(year-2)+'.logfama.txt',year-2))\n",
        "data_train.extend(pre_processing('/content/drive/MyDrive/10-K dataset/all.meta/'+str(year-1)+'.meta.txt','/content/drive/MyDrive/10-K dataset/all.logfama/'+str(year-1)+'.logfama.txt',year-1))\n",
        "train_df = pd.DataFrame(data_train,columns=['token','value'])\n",
        "print(\"Length of training data\",len(data_train))\n",
        "\n",
        "\n",
        "data_test = pre_processing('/content/drive/MyDrive/10-K dataset/all.meta/'+str(year)+'.meta.txt','/content/drive/MyDrive/10-K dataset/all.logfama/'+str(year)+'.logfama.txt',year)\n",
        "test_df = pd.DataFrame(data_test,columns=['token','value'])\n",
        "print(\"Length of testing data\",len(data_test))\n",
        "\n",
        "print(\"SAMPLE INPUT TEXT AND VOLATILITY VALUES\")\n",
        "print(train_df.sample(5)[['token','value']])\n",
        "print(test_df.sample(5)[['token','value']])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of training data 7261\n",
            "Length of testing data 2336\n",
            "SAMPLE INPUT TEXT AND VOLATILITY VALUES\n",
            "                                                  token     value\n",
            "5885  [170, 79, 197, 303, 45, 124, 25, 18, 462, 964,...  -4.44411\n",
            "1187  [170, 79, 197, 303, 45, 124, 25, 18, 251, 121,...  -3.84645\n",
            "5640  [170, 79, 197, 303, 45, 124, 25, 18, 111, 77, ...  -3.42651\n",
            "1977  [170, 79, 197, 303, 45, 124, 25, 18, 181, 435,...  -3.49088\n",
            "821   [170, 79, 197, 303, 45, 124, 25, 18, 181, 435,...  -3.43611\n",
            "                                                  token     value\n",
            "1253  [170, 79, 197, 303, 45, 124, 25, 18, 72, 47, 5...  -3.87449\n",
            "1046  [170, 79, 197, 303, 45, 124, 25, 18, 181, 78, ...  -4.26236\n",
            "1050  [170, 79, 197, 303, 45, 124, 25, 18, 964, 2195...  -2.89789\n",
            "182   [170, 79, 197, 303, 45, 124, 25, 18, 150, 197,...  -4.19876\n",
            "2024  [170, 79, 197, 303, 45, 124, 25, 18, 181, 435,...  -4.47229\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fevays6tkt2p"
      },
      "source": [
        "CNN_train_input = train_df.token.values\n",
        "CNN_train_output = [ float(x) for x in train_df.value.values ]\n",
        "CNN_test_input = test_df.token.values\n",
        "CNN_test_output = [ float(x) for x in test_df.value.values ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PG6qwiQ-kffG"
      },
      "source": [
        "CNN_train_output = np.array(CNN_train_output).reshape(len(CNN_train_output),1)\n",
        "CNN_test_output = np.array(CNN_test_output).reshape(len(CNN_test_output),1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcgQtUmJooLV"
      },
      "source": [
        "**define the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIOmvEheA_Mw"
      },
      "source": [
        "def define_model(max_inp_len,vocabulary_size,embed_dim,filter_sizes,num_filters,pool_size,drop,learning_rate):\n",
        "  \n",
        "  # input and embedding matrix\n",
        "  inputs = Input(shape=(max_inp_len,))\n",
        "  embedding = Embedding(vocabulary_size, embed_dim, weights=[embedding_matrix],trainable = True)(inputs)\n",
        "\n",
        "  custom_objects={'leaky_relu': tf.nn.leaky_relu}\n",
        "\n",
        "  # channel 1 convolution and local max-pooling\n",
        "  convolution_1 = Conv1D(filters=num_filters, kernel_size=filter_sizes[0], activation=custom_objects['leaky_relu'])(embedding)\n",
        "  pool_1 = MaxPooling1D(pool_size=pool_size)(convolution_1)\n",
        "  \n",
        "\t# channel 2 convolution and local max-pooling\n",
        "  convolution_4 = Conv1D(filters=num_filters, kernel_size=filter_sizes[1], activation=custom_objects['leaky_relu'])(embedding)\n",
        "  pool_2 = MaxPooling1D(pool_size=pool_size)(convolution_4)\n",
        "  \n",
        "  # channel 3 convolution and local max-pooling\n",
        "  convolution_5 = Conv1D(filters=num_filters, kernel_size=filter_sizes[2], activation=custom_objects['leaky_relu'])(embedding)\n",
        "  pool_3 = MaxPooling1D(pool_size=pool_size)(convolution_5)\n",
        "  \n",
        "  # merge and dropout\n",
        "  merged = concatenate([pool_1,pool_2,pool_3],axis=1)\n",
        "  drop_out = Dropout(drop)(merged)\n",
        "  flat = Flatten()(drop_out)\n",
        "\n",
        "  # 2 fully connected layers\n",
        "  dense1 = Dense(100, activation=custom_objects['leaky_relu'])(flat)\n",
        "  outputs = Dense(1, activation=custom_objects['leaky_relu'])(dense1)\n",
        "  model = Model(inputs=[inputs], outputs=outputs)\n",
        "    \n",
        "  opt = optimizers.SGD(learning_rate=learning_rate)\n",
        "  model.compile(loss='mse', optimizer=opt)\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3xv22helsjG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ba032fb-3874-4c52-cc43-a6d03820b56b"
      },
      "source": [
        "# define model\n",
        "model = define_model(max_inp_len,vocabulary_size,embed_dim,filter_sizes,num_filters,pool_size,drop,learning_rate)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 20000)]      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 20000, 200)   14085800    input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 19998, 1)     601         embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_4 (Conv1D)               (None, 19997, 1)     801         embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_5 (Conv1D)               (None, 19996, 1)     1001        embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_3 (MaxPooling1D)  (None, 100, 1)       0           conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_4 (MaxPooling1D)  (None, 100, 1)       0           conv1d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_5 (MaxPooling1D)  (None, 100, 1)       0           conv1d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 300, 1)       0           max_pooling1d_3[0][0]            \n",
            "                                                                 max_pooling1d_4[0][0]            \n",
            "                                                                 max_pooling1d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 300, 1)       0           concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 300)          0           dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 100)          30100       flatten_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 1)            101         dense_2[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 14,118,404\n",
            "Trainable params: 14,118,404\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "288-rkCSouut"
      },
      "source": [
        "**Fit the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVCRT8o3LwaS"
      },
      "source": [
        "### applying minmax scalar\n",
        "#from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "#scaler = MinMaxScaler()\n",
        "CNN_train_output = np.array(CNN_train_output).reshape(len(CNN_train_output),1)\n",
        "CNN_test_output = np.array(CNN_test_output).reshape(len(CNN_test_output),1)\n",
        "#output = np.concatenate((CNN_train_output, CNN_test_output))\n",
        "#output = scaler.fit_transform(output)\n",
        "#CNN_train_output = output[:len(CNN_train_input)]\n",
        "#CNN_test_output = output[-len(CNN_test_input):]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Eah3K9ombxX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 735
        },
        "outputId": "a6e77cf4-82d4-4084-cbad-da43264c955c"
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "n_splits = 5\n",
        "data =[]\n",
        "CNN_train_input = np.stack(CNN_train_input)\n",
        "CNN_test_input = np.stack(CNN_test_input)\n",
        "with tf.device('/device:GPU:0'):\n",
        "  kf = KFold(n_splits=n_splits)\n",
        "  fold = 1\n",
        "  for train_index, val_index in kf.split(CNN_train_input):\n",
        "    \n",
        "    checkpoint_filepath = '/content/drive/MyDrive/CNN_Results/CheckPoints/CNN'+str(year)+'_checkpoint_org'+str(fold)\n",
        "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=False,\n",
        "    monitor='loss',\n",
        "    mode='min',\n",
        "    save_best_only=True)\n",
        "\n",
        "    train_history = model.fit(\n",
        "                              CNN_train_input[train_index],\n",
        "                              CNN_train_output[train_index],#output\n",
        "                              epochs=epochs, #epochs\n",
        "                              verbose=1,\n",
        "                              callbacks=[model_checkpoint_callback]\n",
        "                          )\n",
        "    model_best = tf.keras.models.load_model(checkpoint_filepath)\n",
        "    loss_Tr = model_best.evaluate(CNN_train_input[train_index],CNN_train_output[train_index], verbose=0)\n",
        "    loss_Va = model_best.evaluate(CNN_train_input[val_index],CNN_train_output[val_index], verbose=0)\n",
        "    loss_Te = model_best.evaluate(CNN_test_input,CNN_test_output, verbose=0)\n",
        "    data.append({'Training Loss':loss_Tr,'Validation Loss':loss_Va,'Test loss':loss_Te,'year':year,'fold':fold})\n",
        "    fold+=1\n",
        "df = pd.DataFrame(data)\n",
        "df.to_csv(\"CNN_\"+str(year)+\"_results_org.csv\")\n",
        "from google.colab import files\n",
        "files.download(\"CNN_\"+str(year)+\"_results_org.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "182/182 [==============================] - 128s 697ms/step - loss: 10.8048\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/CNN_Results/CheckPoints/CNN2013_checkpoint_org1/assets\n",
            "Epoch 2/30\n",
            "182/182 [==============================] - 126s 692ms/step - loss: 0.4279\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/CNN_Results/CheckPoints/CNN2013_checkpoint_org1/assets\n",
            "Epoch 3/30\n",
            "182/182 [==============================] - 126s 694ms/step - loss: 0.3661\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/CNN_Results/CheckPoints/CNN2013_checkpoint_org1/assets\n",
            "Epoch 4/30\n",
            "182/182 [==============================] - 125s 687ms/step - loss: 0.3536\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/CNN_Results/CheckPoints/CNN2013_checkpoint_org1/assets\n",
            "Epoch 5/30\n",
            "182/182 [==============================] - 125s 687ms/step - loss: 0.3356\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/CNN_Results/CheckPoints/CNN2013_checkpoint_org1/assets\n",
            "Epoch 6/30\n",
            "182/182 [==============================] - 125s 687ms/step - loss: 0.3444\n",
            "Epoch 7/30\n",
            "182/182 [==============================] - 125s 687ms/step - loss: 0.3326\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/CNN_Results/CheckPoints/CNN2013_checkpoint_org1/assets\n",
            "Epoch 8/30\n",
            " 76/182 [===========>..................] - ETA: 1:13 - loss: 0.3182"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-64411adc595a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m                               \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m#epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                               \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                               \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_checkpoint_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m                           )\n\u001b[1;32m     26\u001b[0m     \u001b[0mmodel_best\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}